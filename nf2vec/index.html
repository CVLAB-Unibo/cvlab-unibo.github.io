<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>nf2vec</title>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="icon" type="image/png" href="img/logo_cvlab.png">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <!-- TITLE -->
        <div class="row">
            <h2 class="col-md-18 text-center">
                Deep Learning on Object-centric 3D Neural Fields
                </br>
                <small>
                    Accepted at TPAMI
                </small>
            </h2>
        </div>

        <!-- AUTHORS -->
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://www.unibo.it/sitoweb/pierluigi.zama">
                            Pierluigi Zama Ramirez*
                        </a>
                    </li>
                    <li>
                        <a href="https://scholar.google.it/citations?user=PpHLOpQAAAAJ&hl=en">
                            Luca De Luigi*
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/dsr-lab">
                            Daniele Sirocchi*
                        </a>
                    </li>
                    <br/>
                    <li>
                        <a href="https://www.unibo.it/sitoweb/adriano.cardace2">
                            Adriano Cardace
                        </a>
                    </li>
                    <li>
                        <a href="https://scholar.google.it/citations?user=DYADxJAAAAAJ&hl=en">
                            Riccardo Spezialetti
                        </a>
                    </li>
                    <li>
                        <a href="https://www.unibo.it/sitoweb/francesco.ballerini4">
                            Francesco Ballerini
                        </a>
                    </li>
                    <li>
                        <a href="https://www.unibo.it/sitoweb/samuele.salti">
                            Samuele Salti
                        </a>
                    </li>
                    <li>
                        <a href="https://www.unibo.it/sitoweb/luigi.distefano">
                            Luigi Di Stefano
                        </a>
                    </li>
                </ul>
                * denotes equal contribution
                </br> Department of Computer Science and Engineering (DISI)
                </br> University of Bologna, Italy
            </div>
        </div>

        <!-- LINKS -->
        <div class="row">
            <div class="col-md-8 col-md-offset-2 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="https://arxiv.org/abs/2312.13277">
                            <image src="img/paper_ico.png" height="80px" class="img_border"></image>
                            <h4><strong>Paper</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/CVLAB-Unibo/nf2vec">
                            <image src="img/github_pad.png" height="80px"></image>
                            <h4><strong>Code nf2vec </strong></h4>
                            <h4><strong>(Extended Framework</strong></h4>
                            <h4><strong>for NeRFs)</strong></h4>

                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/CVLAB-Unibo/inr2vec">
                            <image src="img/github_pad.png" height="80px"></image>
                            <h4><strong>Code inr2vec</strong></h4>
                            <h4><strong>(Base Framework)</strong></h4>
                        </a>
                    </li>
                </ul>
            </div>
        </div>

        <!-- ABSTRACT -->
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <image src="img/teaser.png" class="image_center_100" alt="Overview of our framework"></image>
                <p class="text-justify">
                    In recent years, Neural Fields (NFs) have emerged
                    as an effective tool for encoding diverse continuous signals such
                    as images, videos, audio, and 3D shapes. When applied to 3D
                    data, NFs offer a solution to the fragmentation and limitations
                    associated with prevalent discrete representations. However, given
                    that NFs are essentially neural networks, it remains unclear
                    whether and how they can be seamlessly integrated into deep
                    learning pipelines for solving downstream tasks. This paper
                    addresses this research problem and introduces \({\tt nf2vec}\), a
                    framework capable of generating a compact latent representation
                    for an input NF in a single inference pass. We demonstrate
                    that \({\tt nf2vec}\) effectively embeds 3D objects represented by the
                    input NFs and showcase how the resulting embeddings can
                    be employed in deep learning pipelines to successfully address
                    various tasks, all while processing exclusively NFs. We test this
                    framework on several NFs used to represent 3D surfaces, such
                    as unsigned/signed distance and occupancy fields. Moreover, we
                    demonstrate the effectiveness of our approach with more complex
                    NFs that encompass both geometry and appearance of 3D objects
                    such as neural radiance fields.
                </p>
            </div>
        </div>

        </br>

        <!-- METHOD -->
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Method
                </h3>
                <p class="text-justify">
                    Our framework, dubbed \({\tt nf2vec}\), is composed of an encoder and a decoder.
                    The encoder is designed to take as input the weights of a NF and produces a
                    compact embedding that encodes all the relevant information of the input NF. A
                    first challenge in designing an encoder for NFs consists in defining how the
                    encoder should ingest the weights as input, since processing naively all the
                    weights would require a huge amount of memory. 
                    The \({\tt nf2vec}\) encoder is designed with a simple architecture, consisting
                    of a series of linear layers with batch norm and ReLU non-linearity followed by
                    final max pooling. At each stage, the input weight matrix is transformed by one linear
                    layer that applies the same weights to each row of the matrix. The final max
                    pooling compresses all the rows into a single one, obtaining the desired embedding.
                </p>
                <image src="img/encoder.png" class="image_center_100" alt="nf2vec encoder"></image>
                </br>
                <p class="text-justify">
                    In order to guide the \({\tt nf2vec}\) encoder to produce meaningful embeddings,
                    we first note that we are not interested in encoding the values of the input weights
                    in the embeddings produced by our framework, but, rather, in storing information
                    about the 3D object represented by the input NF. For this reason, we supervise
                    the decoder to replicate the function approximated by the input NF instead of
                    directly reproducing its weights, as it would be the case in a standard auto-encoder
                    formulation. In particular, during training, we adopt an implicit decoder which
                    takes in input the embeddings produced by the encoder and decodes the input NF
                    from them. After the overall framework has been trained end to end, the frozen
                    encoder can be used to compute embeddings of unseen NFs with a single forward
                    pass, whereas the implicit decoder can be used, if needed, to reconstruct the discrete
                    representation given an embedding.
                </p>
                <image src="img/framework.png" class="image_center_100" alt="nf2vec framework"></image>
            </div>
        </div>

        </br>

        <!-- RESULTS -->
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Reconstruction
                </h3>
                <p class="text-justify">
                    We compare 3D shapes and NeRFs reconstructed from NFs unseen during training with those reconstructed by the \({\tt nf2vec}\) decoder starting from the latent codes yielded by the encoder. Though our embedding is dramatically more compact than the original NF, the reconstructed discrete data resembles those of the original input NF.
                </p>
                <image src="img/reconstr.png" class="image_center_80" alt="Shape reconstruction"></image>
                <image src="img/reconstr_nerf.png" class="image_center_90" alt="NeRF reconstruction"></image>
                <h3>
                    Interpolation
                </h3>
                <p class="text-justify">
                    We linearly interpolate between two object embeddings produced by \({\tt nf2vec}\). Results highlight that the learned latent spaces enable smooth interpolations between shapes represented as NFs.
                </p>
                <image src="img/interp.png" class="image_center_80" alt="Shape interpolation"></image>
                </br>
                <image src="img/interp_nerf.png" class="image_center_80" alt="NeRF interpolation"></image>
                </br>
                <p class="text-justify">
                    Additionally, given two input NeRFs, we render images from networks obtained by interpolating their weights. We compare these results with those obtained from the interpolation of \({\tt nf2vec}\) embeddings. Notably, renderings obtained by averaging the weights of the two NeRFs appear blurred and lack 3D structure, whereas renderings produced by \({\tt nf2vec}\) preserve details and maintain 3D consistency.
                </p>
                <image src="img/interp_nerf_weights.png" class="image_center_90" alt="NeRF weights interpolation"></image>
                <h3>
                    Retrieval
                </h3>
                <p class="text-justify">
                    We perform shape retrieval by computing the Euclidean distance between \({\tt nf2vec}\) embeddings of unseen point clouds from the test set. The retrieved shapes not only belong to the same class as the query but also exhibit similar coarse structures.
                </p>
                <image src="img/retrieval.png" class="image_center_70" alt="Point cloud retrieval"></image>
                </br>
                <p class="text-justify">
                    Performing the same experiment on NeRFs allows to retrieve neighbors that are similar to the query in both geometry and color.
                </p>
                <image src="img/retrieval_nerf.png" class="image_center_100" alt="NeRF retrieval"></image>
                <h3>
                    Part segmentation
                </h3>
                <p class="text-justify">
                    Part segmentation aims to predict a semantic (i.e. part) label for each point of a given cloud. We tackle this problem by training a decoder similar to that used to train our framework. Such decoder is fed with the \({\tt nf2vec}\) embedding of the NF representing the input cloud, concatenated with the coordinate of a 3D query, and it is trained to predict the label of the query point. Notice how the \({\tt nf2vec}\) embeddings, although task-agnostic, allow to perform a local discriminative task such as part segmentation. 
                </p>
                <image src="img/partseg.png" class="image_center_100" alt="Part segmentation"></image>
                <h3>
                    Generation
                </h3>
                <p class="text-justify">
                    We employ a Latent-GAN to generate embeddings resembling those produced by \({\tt nf2vec}\) from random noise. Generated embeddings can be decoded into discrete representations using the implicit decoder from the \({\tt nf2vec}\) training. As our framework is agnostic towards the original discrete representation of shapes used to learn the NFs, we can train Latent-GANs with embeddings representing point clouds or meshes based on the same identical protocol and architecture. Furthermore, by generating embeddings representing NFs, our method allows point cloud sampling at any arbitrary resolution, whereas SP-GAN needs a new training for each desired resolution.
                </p>
                <image src="img/gen.png" class="image_center_100" alt="Shape generation"></image>
                </br>
                <p class="text-justify"></p>
                    When applied to NeRFs, our generation method produces renderings that have a good level of realism and diversity. Notably, the 3D consistency of images obtained from different viewpoints is preserved.
                </p>
                <image src="img/gen_nerf.png" class="image_center_80" alt="NeRF generation"></image>
                <h3>
                    Learning a mapping between \({\tt nf2vec}\) embedding spaces
                </h3>
                <p class="text-justify">
                    We develop a transfer function specifically designed to operate on \({\tt nf2vec}\) embeddings as both input and output data. Such transfer function can be realized by a simple MLP that maps the input embedding into the output one and is trained with standard MSE loss. We explore two tasks: first, we address point cloud completion by learning a mapping from \({\tt nf2vec}\) embeddings of NFs that represent incomplete clouds to embeddings associated with complete clouds. Then, we tackle the task of surface reconstruction by training the transfer function to map \({\tt nf2vec}\) embeddings representing point clouds into embeddings that can be decoded into meshes. Indeed, by processing exclusively NF embeddings, we can obtain output shapes that are highly compatible with the input ones while preserving their distinctive details, like the pointy wings of an airplane or the flap of a car.
                </p>
                <image src="img/map.png" class="image_center_100" alt="Shape mapping"></image>
                </br>
                <p class="text-justify">
                    We also show that the same methodology allows to learn a transfer function that maps \({\tt nf2vec}\) embeddings of point clouds to \({\tt nf2vec}\) embeddings of NeRFs. The generated NeRFs preserve the geometry of the input shapes and exhibit plausible diverse colors associated with different object parts.
                </p>
                <image src="img/map_nerf.png" class="image_center_70" alt="NeRF mapping"></image>
            </div>
        </div>

        <!-- CITATION -->
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Cite us
                </h3>
                <div class="form-group col-md-18 col-md-offset-0">
                    <pre>
@article{ramirez2023nf2vec,
    title = {Deep Learning on 3D Neural Fields},
    author = {Zama Ramirez, Pierluigi 
              and De Luigi, Luca 
              and Sirocchi, Daniele 
              and Cardace, Adriano 
              and Spezialetti, Riccardo 
              and Ballerini, Francesco 
              and Salti, Samuele 
              and Di Stefano, Luigi},
    journal = {IEEE Transactions on Pattern Analysis &amp; Machine Intelligence},
    year = {2024}
}
                    </pre>
                </div>
            </div>
        </div>
    
    </div>
</body>

</html>
