<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>nf2vec</title>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="icon" type="image/png" href="img/logo_cvlab.png">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <!-- TITLE -->
        <div class="row">
            <h2 class="col-md-18 text-center">
                Deep Learning on 3D Neural Fields
                </br>
                <small>
                    Under Review
                </small>
            </h2>
        </div>

        <!-- AUTHORS -->
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://www.unibo.it/sitoweb/pierluigi.zama">
                            Pierluigi Zama Ramirez*
                        </a>
                    </li>
                    <li>
                        <a href="https://scholar.google.it/citations?user=PpHLOpQAAAAJ&hl=en">
                            Luca De Luigi*
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/dsr-lab">
                            Daniele Sirocchi*
                        </a>
                    </li>
                    <br/>
                    <li>
                        <a href="https://www.unibo.it/sitoweb/adriano.cardace2">
                            Adriano Cardace
                        </a>
                    </li>
                    <li>
                        <a href="https://scholar.google.it/citations?user=DYADxJAAAAAJ&hl=en">
                            Riccardo Spezialetti
                        </a>
                    </li>
                    <li>
                        <a href="https://www.unibo.it/sitoweb/francesco.ballerini4">
                            Francesco Ballerini
                        </a>
                    </li>
                    <li>
                        <a href="https://www.unibo.it/sitoweb/samuele.salti">
                            Samuele Salti
                        </a>
                    </li>
                    <li>
                        <a href="https://www.unibo.it/sitoweb/luigi.distefano">
                            Luigi Di Stefano
                        </a>
                    </li>
                </ul>
                * denotes equal contribution
                </br> Department of Computer Science and Engineering (DISI)
                </br> University of Bologna, Italy
            </div>
        </div>

        <!-- LINKS -->
        <div class="row">
            <div class="col-md-8 col-md-offset-2 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="https://arxiv.org/abs/2312.13277">
                            <image src="img/paper_ico.png" height="80px" class="img_border"></image>
                            <h4><strong>Paper</strong></h4>
                            <h4><strong>(under review)</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/CVLAB-Unibo/nf2vec">
                            <image src="img/github_pad.png" height="80px"></image>
                            <h4><strong>Code nf2vec </strong></h4>
                            <h4><strong>(Extended Framework</strong></h4>
                            <h4><strong>for NeRFs)</strong></h4>

                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/CVLAB-Unibo/inr2vec">
                            <image src="img/github_pad.png" height="80px"></image>
                            <h4><strong>Code inr2vec</strong></h4>
                            <h4><strong>(Base Framework)</strong></h4>
                        </a>
                    </li>
                </ul>
            </div>
        </div>

        <!-- ABSTRACT -->
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <image src="img/teaser.png" class="image_center_100" alt="Overview of our framework"></image>
                <p class="text-justify">
                    In recent years, Neural Fields (NFs) have emerged
                    as an effective tool for encoding diverse continuous signals such
                    as images, videos, audio, and 3D shapes. When applied to 3D
                    data, NFs offer a solution to the fragmentation and limitations
                    associated with prevalent discrete representations. However, given
                    that NFs are essentially neural networks, it remains unclear
                    whether and how they can be seamlessly integrated into deep
                    learning pipelines for solving downstream tasks. This paper
                    addresses this research problem and introduces nf2vec, a
                    framework capable of generating a compact latent representation
                    for an input NF in a single inference pass. We demonstrate
                    that nf2vec effectively embeds 3D objects represented by the
                    input NFs and showcase how the resulting embeddings can
                    be employed in deep learning pipelines to successfully address
                    various tasks, all while processing exclusively NFs. We test this
                    framework on several NFs used to represent 3D surfaces, such
                    as unsigned/signed distance and occupancy fields. Moreover, we
                    demonstrate the effectiveness of our approach with more complex
                    NFs that encompass both geometry and appearance of 3D objects
                    such as neural radiance fields.
                </p>
            </div>
        </div>

        </br>
        </br>

        <!-- METHOD -->
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Method
                </h3>

                <p class="text-justify">
                    Our framework, dubbed \({\tt nf2vec}\), is composed of an encoder and a decoder.
                    The encoder is designed to take as input the weights of an NF and produce a
                    compact embedding that encodes all the relevant information of the input NF. A
                    first challenge in designing an encoder for NFs consists in defining how the
                    encoder should ingest the weights as input, since processing naively all the
                    weights would require a huge amount of memory. 
                    The \({\tt nf2vec}\) encoder is designed with a simple architecture, consisting
                    of a series of linear layers with batch norm and ReLU non-linearity followed by
                    final max pooling. At each stage, the input weight matrix is transformed by one linear
                    layer, that applies the same weights to each row of the matrix. The final max
                    pooling compresses all the rows into a single one, obtaining the desired embedding.
                </p>
                <image src="img/encoder.png" class="image_center_100" alt="nf2vec encoder"></image>

                </br>
                </br>

                <p class="text-justify">
                    In order to guide the \({\tt nf2vec}\) encoder to produce meaningful embeddings,
                    we first note that we are not interested in encoding the values of the input weights
                    in the embeddings produced by our framework, but, rather, in storing information
                    about the 3D object represented by the input NF. For this reason, we supervise
                    the decoder to replicate the function approximated by the input NF instead of
                    directly reproducing its weights, as it would be the case in a standard auto-encoder
                    formulation. In particular, during training, we adopt an implicit decoder which
                    takes in input the embeddings produced by the encoder and decodes the input NFs
                    from them. After the overall framework has been trained end to end, the frozen
                    encoder can be used to compute embeddings of unseen NFs with a single forward
                    pass while the implicit decoder can be used, if needed, to reconstruct the discrete
                    representation given an embedding.
                </p>
                <image src="img/framework.png" class="image_center_100" alt="nf2vec framework"></image>

                </br>
                </br>

                <!-- <p class="text-justify">
                    In the following figure we compare 3D shapes reconstructed from NFs unseen during
                    training with those reconstructed by the \({\tt nf2vec}\) decoder starting from
                    the latent codes yielded by the encoder. We visualize point clouds with 8192 points,
                    meshes reconstructed by marching cubes from a grid with resolution \(128^3\) and
                    voxels with resolution \(64^3\). We note that, though our embedding is dramatically
                    more compact than the original NF, the reconstructed shape resembles the ground-truth
                    with a good level of detail.
                </p>
                <image src="img/NF2vec_io.png" class="image_center_80" alt="nf2vec reconstructions"></image>

                </br>
                </br> -->
<!-- 
                <p class="text-justify">
                    Moreover, we linearly interpolate between the embeddings produced by \({\tt nf2vec}\)
                    from two input shapes and show the shapes reconstructed from the interpolated
                    embeddings. The results presented in the next figure highlight that the latent
                    space learned by \({\tt nf2vec}\) enables smooth interpolations between shapes
                    represented as NFs.
                </p>
                <image src="img/interpolation.png" class="image_center_80" alt="nf2vec interpolations"></image> -->

            </div>
        </div>

        </br>
        </br>

        <!-- RESULTS -->
        <!--
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results
                </h3>

                <p class="text-justify">
                    We first evaluate the feasibility of using \({\tt nf2vec}\) embeddings of NFs
                    to solve tasks usually tackled by representation learning, and we select 3D
                    retrieval as a benchmark. Quantitative results, reported in the following table,
                    show that, while there is an average gap of 1.8 mAP with PointNet++, \({\tt nf2vec}\)
                    is able to match, and in some cases even surpass, the performance of the other baselines.
                </p>
                <image src="img/tab_retrieval.png" class="image_center_100" alt="retrieval"></image>

                </br>
                </br>

                <p class="text-justify">
                    We then address the problem of classifying point clouds, meshes and voxel grids.
                    Despite the different nature of the discrete representations taken into account,
                    \({\tt nf2vec}\) allows us to perform shape classification on NFs embeddings by
                    the very same downstream network architecture, i.e., a simple fully connected
                    classifier consisting of three layers with 1024, 512 and 128 features. The results
                    reported in the next table show that \({\tt nf2vec}\) embeddings deliver
                    classification accuracy close to the specialized baselines across all the
                    considered datasets, regardless of the original discrete representation of the
                    shapes in each dataset. Remarkably, our framework allows us to apply the same
                    simple classification architecture on all the considered input modalities, in stark
                    contrast with all the baselines that are highly specialized for each modality,
                    exploit inductive biases specific to each such modality and cannot be deployed on
                    representations different from those they were designed for.
                </p>
                <image src="img/tab_classification.png" class="image_center_80" alt="classification"></image>

                </br>
                </br>

                <p class="text-justify">
                    While the tasks of retrieval and classification concern the possibility of using
                    \({\tt nf2vec}\) embeddings as a compact proxy for the global information of the
                    input shapes, with the task of point cloud part segmentation we aim at investigating
                    whether \({\tt nf2vec}\) embeddings can be used also to assess upon local properties
                    of shapes. The part segmentation task consists in predicting a semantic label for
                    each point of a given cloud. We tackle this problem by training a decoder similar
                    to that used to train our framework (see left part of the following figure). Such
                    decoder is fed with the \({\tt nf2vec}\) embedding of the NF representing the
                    input cloud, concatenated with the coordinate of a 3D query, and it is trained to
                    predict the label of the query point. Qualitative results reported in the right
                    part of the figure show the possibility of performing also a local discriminative
                    task as challenging as part segmentation based on the task-agnostic embeddings
                    produced by \({\tt nf2vec}\).
                </p>
                <image src="img/partseg.png" class="image_center_100" alt="part segmentation"></image>

                </br>
                </br>

                <p class="text-justify">
                    We also address the task of shape generation in an adversarial setting to investigate
                    whether the compact representations produced by our framework can be adopted also
                    as medium for the output of deep learning pipelines. For this purpose, as depicted
                    in the next figure, we train a Latent-GAN to generate embeddings indistinguishable
                    from those produced by \({\tt nf2vec}\) starting from random noise. The generated
                    embeddings can then be decoded into discrete representations with the implicit
                    decoder exploited during \({\tt nf2vec}\) training. Since our framework is agnostic
                    w.r.t. the original discrete representation of shapes used to fit NFs, we can
                    train Latent-GANs with embeddings representing point clouds or meshes based on the
                    same identical protocol and architecture (two simple fully connected networks as
                    generator and discriminator). In the following figure, we show some samples
                    generated with the described procedure, comparing them with SP-GAN for what concerns
                    point clouds and Occupancy Networks (VAE formulation) for meshes. The shapes generated
                    with our Latent-GAN trained only on \({\tt nf2vec}\) embeddings seem comparable
                    to those produced by the considered baselines, in terms of both diversity and richness of details.
                </p>
                <image src="img/generative.png" class="image_center_100" alt="shape generation"></image>

                We also perform the task of NeRF generation using the methodology described above.

                <image src="img/generative_nerf.png" class="image_center_100" alt="mapping"></image>

                </br>
                </br>

                <p class="text-justify">
                    Finally we consider the possibility of learning a mapping between two distinct
                    latent spaces produced by our framework for two separate datasets of NFs, based
                    on a \(\textit{transfer}\) function designed to operate on \({\tt nf2vec}\)
                    embeddings as both input and output data. Such transfer function can be realized
                    by a simple fully connected network that maps the input embedding into the output
                    one and is trained by a standard MSE loss. Here, we apply this idea to two tasks.
                    Firstly, we address point cloud completion by learning a mapping from \({\tt nf2vec}\)
                    embeddings of NFs that represent incomplete clouds to embeddings associated with
                    complete clouds. Then, we tackle the task of surface reconstruction on ShapeNet cars,
                    training the transfer function to map \({\tt nf2vec}\) embeddings representing
                    point clouds into embeddings that can be decoded into meshes. As we can appreciate
                    from the samples reported in the figure, the transfer function can learn an effective
                    mapping between \({\tt nf2vec}\) latent spaces. Indeed, by processing exclusively
                    NFs embedding, we can obtain output shapes that are highly compatible with the
                    input ones whilst preserving the distinctive details, like the pointy wing of the
                    airplane or the flap of the first car.
                </p>
                <image src="img/mapping.png" class="image_center_100" alt="mapping"></image>

                Finally, we address the task of mapping a UDF into 
                a NeRF. The task is difficult as we need to preserve the geometry while introducing a
                realistic appeareance, yet we can address with our feature mapping strategy.

                <image src="img/mapping_nerf.png" class="image_center_100" alt="mapping"></image>

            </div>
        </div>

        </br>
        </br>-->

        <!-- CITATION -->
        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Cite us
                </h3>
                <div class="form-group col-md-18 col-md-offset-0">
                    <pre>
@inproceedings{deluigi2023nf2vec,
    title = {Deep Learning on Implicit Neural Representations of Shapes},
    author = {De Luigi, Luca 
              and Cardace, Adriano 
              and Spezialetti, Riccardo 
              and Zama Ramirez, Pierluigi 
              and Salti, Samuele
              and Di Stefano, Luigi},
    booktitle = {International Conference on Learning Representations (ICLR)},
    year = {2023}
}
                    </pre>
                </div>
            </div>
        </div> -->

    </div>
</body>

</html>
